{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tqdm import tqdm\n",
    "import import_ipynb\n",
    "from my_layers import Average, WeightedSum, WeightedAspectEmb, MaxMargin, Attention\n",
    "from reader import get_data\n",
    "from w2vEmbReader import W2VEmbReader as EmbReader\n",
    "from model import create_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'ortho_reg': 0.1,\n",
    "    'neg_size': 20,\n",
    "    'emb_dim': 200,\n",
    "    'aspect_size': 14 ,\n",
    "    'emb_path': '../w2v_embedding',\n",
    "    'algorithm': 'adam',\n",
    "    'batch_size': 32,\n",
    "    'neg_size': 20,\n",
    "    'epochs' : 15\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = '../' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimizers import get_optimizer\n",
    "optimizer = get_optimizer(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab, train_x, overall_maxlen = get_data('clean_review', 9000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = sequence.pad_sequences(train_x, maxlen=overall_maxlen)\n",
    "# test_x = sequence.pad_sequences(test_x, maxlen=overall_maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "batches_per_epoch = 1000\n",
    "min_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_batch_generator(data, batch_size):\n",
    "    n_batch = len(data) / batch_size\n",
    "    batch_count = 0\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    while True:\n",
    "        if batch_count == n_batch:\n",
    "            np.random.shuffle(data)\n",
    "            batch_count = 0\n",
    "\n",
    "        batch = data[batch_count*batch_size: (batch_count+1)*batch_size]\n",
    "        batch_count += 1\n",
    "        yield batch\n",
    "\n",
    "def negative_batch_generator(data, batch_size, neg_size):\n",
    "    data_len = data.shape[0]\n",
    "    dim = data.shape[1]\n",
    "\n",
    "    while True:\n",
    "        indices = np.random.choice(data_len, batch_size * neg_size)\n",
    "        samples = data[indices].reshape(batch_size, neg_size, dim)\n",
    "        yield samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_margin_loss(y_true, y_pred):\n",
    "    return K.mean(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(args, overall_maxlen, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer('word_emb').trainable=False\n",
    "model.compile(optimizer=optimizer, loss=max_margin_loss, metrics=[max_margin_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_inv = {}\n",
    "for w, ind in vocab.items():\n",
    "    vocab_inv[ind] = w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_gen = sentence_batch_generator(train_x, args['batch_size'])\n",
    "neg_gen = negative_batch_generator(train_x, args['batch_size'], args['neg_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.keras.utils.plot_model(\n",
    "#     model,\n",
    "#     to_file=\"model.png\",\n",
    "#     show_shapes=False,\n",
    "#     show_layer_names=True,\n",
    "#     rankdir=\"TB\",\n",
    "#     expand_nested=True,\n",
    "#     dpi=96,\n",
    "# ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_per_epoch = 100\n",
    "min_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(args['epochs']):\n",
    "    loss, max_margin_loss = 0., 0.\n",
    "    \n",
    "    for b in tqdm(range(batches_per_epoch)):\n",
    "        sen_input = next(sen_gen)\n",
    "        neg_input = next(neg_gen)\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "            batch_loss, batch_max_margin_loss = model.train_on_batch([sen_input, neg_input], np.ones((args['batch_size'], 1)))\n",
    "        \n",
    "            loss += batch_loss / batches_per_epoch\n",
    "            max_margin_loss += batch_max_margin_loss / batches_per_epoch\n",
    "        \n",
    "    if loss < min_loss:\n",
    "        min_loss = loss\n",
    "        word_emb = model.get_layer('word_emb').get_weights()\n",
    "        aspect_emb = model.get_layer('aspect_emb').get_weights()\n",
    "        word_emb = word_emb / np.linalg.norm(word_emb, axis=-1, keepdims=True)\n",
    "        aspect_emb = aspect_emb / np.linalg.norm(aspect_emb, axis=-1, keepdims=True)\n",
    "        aspect_file = codecs.open(out_dir+'/aspect.log', 'w', 'utf-8')\n",
    "        model.save_weights(out_dir+'/model_param')\n",
    "        \n",
    "        for ind in range(len(aspect_emb[0])):\n",
    "            desc = aspect_emb[0][ind]\n",
    "            sims = word_emb[0].dot(desc.T)\n",
    "            ordered_words = np.argsort(sims)[::-1]\n",
    "            \n",
    "            desc_list = [vocab_inv[w] for w in ordered_words[:20]]\n",
    "            aspect_file.write('Aspect %d:\\n' % ind)\n",
    "            aspect_file.write(' '.join(desc_list) + '\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
