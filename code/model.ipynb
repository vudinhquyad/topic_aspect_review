{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Layer, Dense, Activation\n",
    "\n",
    "import import_ipynb\n",
    "from my_layers import Attention, Average, WeightedSum, WeightedAspectEmb, MaxMargin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(args, maxlen, vocab):\n",
    "    \n",
    "    @tf.keras.utils.register_keras_serializable(package='Custom', name='l1')\n",
    "    def ortho_reg(weight_matrix):\n",
    "        ### orthogonal regularization for aspect embedding matrix ###\n",
    "        w_n = weight_matrix / K.cast(K.epsilon() + K.sqrt(K.sum(K.square(weight_matrix), axis=-1, keepdims=True)), K.floatx())\n",
    "#         reg = K.sum(K.square(K.dot(w_n, K.transpose(w_n)) - K.eval(K.eye(w_n.shape[0]))))\n",
    "#         reg = K.sum(K.square(K.dot(w_n, K.transpose(w_n)) - K.eye(w_n.shape[0])))\n",
    "#         print(reg)\n",
    "#         return args['ortho_reg']*reg\n",
    "#         print(weight_matrix)\n",
    "        return (0.01 * tf.math.reduce_sum(tf.math.abs(weight_matrix)))\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "    ##### Inputs #####\n",
    "\n",
    "    sentence_input = Input(shape=(maxlen,), dtype='int32', name='sentence_input')\n",
    "    neg_input = Input(shape=(args['neg_size'], maxlen), dtype='int32', name='neg_input')\n",
    "\n",
    "    ##### Construct word embedding layer #####\n",
    "    word_emb = Embedding(vocab_size, args['emb_dim'], mask_zero=True, name='word_emb')\n",
    "\n",
    "    ##### Compute sentence representation #####\n",
    "    e_w = word_emb(sentence_input)\n",
    "    y_s = Average()(e_w)\n",
    "    att_weights = Attention(name='att_weights')([e_w, y_s])\n",
    "    z_s = WeightedSum()([e_w, att_weights])\n",
    "\n",
    "    ##### Compute representations of negative instances #####\n",
    "    e_neg = word_emb(neg_input)\n",
    "    z_n = Average()(e_neg)\n",
    "\n",
    "    ##### Reconstruction #####\n",
    "    p_t = Dense(args['aspect_size'])(z_s)\n",
    "    p_t = Activation('softmax', name='p_t')(p_t)\n",
    "#     r_s = WeightedAspectEmb(args['aspect_size'], args['emb_dim'], name='aspect_emb',\n",
    "#             kernel_regularizer=tf.keras.regularizers.L1(0.1))(p_t)\n",
    "    \n",
    "    r_s = WeightedAspectEmb(args['aspect_size'], args['emb_dim'], name='aspect_emb',\n",
    "            kernel_regularizer=ortho_reg)(p_t)\n",
    "\n",
    "\n",
    "    ##### Loss #####\n",
    "    loss = MaxMargin(name='max_margin')([z_s, z_n, r_s])\n",
    "    model = Model(inputs=[sentence_input, neg_input], outputs=loss)\n",
    "\n",
    "    ### Word embedding and aspect embedding initialization ######\n",
    "    if args['emb_path']:\n",
    "        from w2vEmbReader import W2VEmbReader as EmbReader\n",
    "        emb_reader = EmbReader(args['emb_path'], emb_dim=args['emb_dim'])\n",
    "        model.get_layer('word_emb').embeddings_initializer = [emb_reader.get_emb_matrix_given_vocab(\n",
    "                            vocab, model.get_layer('word_emb').get_weights()[0])]\n",
    "\n",
    "        model.get_layer('aspect_emb').set_weights([emb_reader.get_aspect_matrix(args['aspect_size'])])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
